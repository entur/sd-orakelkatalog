{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Søk i data-norge ved hjelp av LLM\n"
   ],
   "metadata": {
    "id": "fJ0U2anlkv8M"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Del 1: Oppsett"
   ],
   "metadata": {
    "id": "1_mlDqlok4YC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Oppsett-biten av denne notebooken trenger kun å gjøres en gang. Første steg er å sette opp konfig for GCP prosjekt. Denne er nå satt opp mot TDS sitt prosjekt i Entur sin GCP."
   ],
   "metadata": {
    "id": "VcOjjQVRZPUv"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ckUcDHPbZMpo"
   },
   "outputs": [],
   "source": [
    "project_id = os.getenv('GCP_PROJECT_ID')\n",
    "region = os.getenv('GCP_REGION')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vi må også sette opp konfig for Cloud SQL. Vi har en instance i prosjektet som kjører en PostgreSQL server."
   ],
   "metadata": {
    "id": "NHOOclw4g0LP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Cloud SQL info\n",
    "instance_name = os.getenv('SQL_INSTANCE_ID')\n",
    "database_user = os.getenv('SQL_DB_USER')\n",
    "database_password = os.getenv('SQL_PASSWORD')\n",
    "database_name = \"fdk-v1\""
   ],
   "metadata": {
    "id": "LZXNMO9jZ76G"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sett opp søke-variabler. Disse variablene styrer søket i PostgreSQL databasen. Disse kan også endres per søk, se lenger ned i notebooken for eksempel.\n",
    "\n",
    "Num_matches er max antall treff returnert fra query.\n",
    "\n",
    "Similarity_threshold er hvor likt et datasett må matche søk for å returneres. Må være en verdi mellom 0 og 1."
   ],
   "metadata": {
    "id": "q_Ovf3Vmg8Z3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Query info\n",
    "num_matches = 10\n",
    "similarity_threshold = 0.5"
   ],
   "metadata": {
    "id": "uMxNLBVVZ_QR"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importer nødvendige pakker. Enkelte av disse må først installeres i python-miljøet"
   ],
   "metadata": {
    "id": "hC6g5APycQoj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install langchain\n",
    "!pip install langchain_google_vertexai\n",
    "!pip install openai\n",
    "!pip install cohere\n",
    "!pip install tiktoken\n",
    "!pip install asyncpg\n",
    "!pip install pgvector\n",
    "!pip install cloud-sql-python-connector"
   ],
   "metadata": {
    "id": "_GMwC-08q-lp",
    "outputId": "17ce7f93-4af4-4607-ed65-95d375c95a08",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_google_vertexai import VertexAIEmbeddings, VertexAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from google.cloud.sql.connector import Connector\n",
    "from pgvector.asyncpg import register_vector\n",
    "from google.cloud import aiplatform\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import asyncpg"
   ],
   "metadata": {
    "id": "deCKdOdaaQHs"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Nå må vi initialisere prosjektet for bruk av AI.\n"
   ],
   "metadata": {
    "id": "uvAoKXCkkpE8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "aiplatform.init(project=f\"{project_id}\", location=f\"{region}\")"
   ],
   "metadata": {
    "id": "zFRIlsX0alK4"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dette er vår hovedfunksjon for å finne liknende datasett ut ifra en query. Vi bruker et kosinus-søk for å finne liknende datasett. Disse datasettene blir senere matet inn i en LLM som bruker de til å besvare spørsmålet på en fornuftig måte."
   ],
   "metadata": {
    "id": "yTQLFTbAmQ6_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "async def main(qe):\n",
    "    matches = []\n",
    "    loop = asyncio.get_running_loop()\n",
    "    async with Connector(loop=loop) as connector:\n",
    "        # Create connection to Cloud SQL database.\n",
    "        conn: asyncpg.Connection = await connector.connect_async(\n",
    "            f\"{project_id}:{region}:{instance_name}\",  # Cloud SQL instance connection name\n",
    "            \"asyncpg\",\n",
    "            user=f\"{database_user}\",\n",
    "            password=f\"{database_password}\",\n",
    "            db=f\"{database_name}\",\n",
    "        )\n",
    "        await register_vector(conn)\n",
    "\n",
    "        # Use cosine similarity search to find the top five products\n",
    "        # that are most closely related to the input query.\n",
    "\n",
    "        results = await conn.fetch(\"\"\"\n",
    "                     WITH vector_matches AS (\n",
    "                             SELECT id,\n",
    "                                    1 - (embedding <=> $1) AS similarity\n",
    "                             FROM dataset_embeddings\n",
    "                             WHERE 1 - (embedding <=> $1) > $2\n",
    "                             ORDER BY similarity DESC\n",
    "                             LIMIT $3\n",
    "                     )\n",
    "                     SELECT id,\n",
    "                            summary\n",
    "                     FROM datasets\n",
    "                     WHERE id IN (SELECT id FROM vector_matches)\n",
    "                     \"\"\",\n",
    "                                   qe, similarity_threshold, num_matches)\n",
    "\n",
    "        if len(results) == 0:\n",
    "            print(\"Dessverre fant vi ikke noe matchende datasett.\")\n",
    "\n",
    "        for r in results:\n",
    "            # Collect the description for all the matched similar toy products.\n",
    "            matches.append(r[\"summary\"])\n",
    "\n",
    "        await conn.close()\n",
    "        return matches"
   ],
   "metadata": {
    "id": "14tEqVNrcR_y"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Under er instruksjonene vi gir til LLM modellen for å få gode svar. Du må gjerne eksperimentere litt for å prøve å få bedre svar.\n",
    "\n",
    "User query her er spørsmålet som stilles til LLM-en.\n",
    "\n",
    "Description vil være en oppsummering av alle datasettene som returneres fra søket mot PostgreSQL."
   ],
   "metadata": {
    "id": "2EMQTMF0m4h-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "map_prompt_template = \"\"\"\n",
    "\n",
    "\n",
    "              You will be given a detailed description of a published dataset in norwegian.\n",
    "              This description is enclosed in triple backticks (```).\n",
    "              Using this description only, extract the title of the dataset,\n",
    "              the category and its most useful features.\n",
    "\n",
    "              ```{text}```\n",
    "              SUMMARY:\n",
    "              \"\"\"\n",
    "\n",
    "combine_prompt_template = \"\"\"\n",
    "                You will be given a detailed description of different datasets in norwegian\n",
    "                enclosed in triple backticks (```) and a question enclosed in\n",
    "                double backticks(``).\n",
    "                Select the datasets that is most relevant to answer the question.\n",
    "                Using that selected dataset description, answer the following\n",
    "                question in as much detail as possible.\n",
    "                You should only use the information in the description.\n",
    "                Your answer should include the title of the dataset, and why it matches the question posed by the user.\n",
    "                Structure your answer to be easily readable by a human person.\n",
    "                Do not answer in markdown format.\n",
    "                If no datasets are given, assume there are no datasets matching the\n",
    "                question, and explain that the data might not exist.\n",
    "                Try to refer to the user directly.\n",
    "                Give the answer in Norwegian.\n",
    "\n",
    "                Description:\n",
    "                ```{text}```\n",
    "\n",
    "\n",
    "                Question:\n",
    "                ``{user_query}``\n",
    "\n",
    "\n",
    "                Answer:\n",
    "                \"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\", \"user_query\"])"
   ],
   "metadata": {
    "id": "L-dDwRBhc38h"
   },
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "For å kunne koble deg til GCP-projektet vårt må du først autentisere deg gjennom Google. Dette gjøres via en enkel innlogging som startes automatisk når koden under kjøres. Google-brukeren din må ha tilgang til GCP-prosjektet\n",
    "\n",
    "```\n",
    "ent-data-fdkllm-ext-dev\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "Y01iF2kFnzri"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ],
   "metadata": {
    "id": "UHs6SYbCe2pk"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dette er siste del av oppsettet. Vi starter opp Vertex AI som er LLM-modellen vi bruker."
   ],
   "metadata": {
    "id": "1O0rzc-6ofr4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "llm = VertexAI(model_name=\"text-bison\", max_output_tokens=1000)\n",
    "chain = load_summarize_chain(llm,\n",
    "                             chain_type=\"map_reduce\",\n",
    "                             map_prompt=map_prompt,\n",
    "                             combine_prompt=combine_prompt)"
   ],
   "metadata": {
    "id": "S6rlep3YdKAP"
   },
   "execution_count": 60,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Del 2: Interaksjon mot modellen\n",
    "\n",
    "Nå skal vi endelig gjøre spørringer mot modellen vår.\n",
    "Alt du trenger å gjøre er å oppgi et spørsmål eller beskrivelse under 'user_query' og kjøre de tre neste kodeblokkene i sekvens. Du må gjerne også eksperimentere med de to andre variablene, men disse må ikke endres hvis du ikke føler for det."
   ],
   "metadata": {
    "id": "1DmJFiEjowN7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# @markdown Vennligst fyll ut spørsmål eller beskrivelse under.\n",
    "# Please fill in these values.\n",
    "user_query = \"\\\"Har du data om tog i Bergen?\\\"\\\"\"  # @param {type:\"string\"}\n",
    "num_matches = 5  # @param {type:\"integer\"}\n",
    "similarity_threshold = 0.5  # @param {type:\"number\"}\n",
    "\n",
    "# Quick input validations.\n",
    "assert user_query, \"⚠️ Please input a valid input search text\"\n",
    "\n",
    "# Generate vector embedding for the user query.\n",
    "embeddings_service = VertexAIEmbeddings(model_name=\"textembedding-gecko-multilingual@001\")\n",
    "qe = embeddings_service.embed_query(user_query)\n",
    "matches = await main(qe)\n",
    "\n",
    "docs = [Document(page_content=t) for t in matches]\n",
    "\n",
    "answer = chain.invoke({\n",
    "    'input_documents': docs,\n",
    "    'user_query': user_query,\n",
    "}, return_only_outputs=True)\n",
    "\n",
    "print(f\"QUESTION: {user_query}\")\n",
    "print(f\"ANSWER: {answer['output_text']}\")"
   ],
   "metadata": {
    "id": "NvmnDVj4hHtY",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "be793421-a603-4b37-eb9f-a9473ea2aba5"
   },
   "execution_count": 62,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/google/cloud/sql/connector/refresh_utils.py:214: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to not_valid_after_utc.\n",
      "  expiration = x509.not_valid_after\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "QUESTION: \"Har du data om tog i Bergen?\"\"\n",
      "ANSWER:  Hei,\n",
      "\n",
      "                Det finnes et datasett som heter \"Trafikkdata fra Jernbaneverket\" som kan være relevant for spørsmålet ditt. Dette datasettet inneholder rutetider og sanntidsinformasjon om togtrafikken i Norge, inkludert Bergen.\n",
      "\n",
      "                Datasettet er kategorisert som \"Ukjent\", og de mest nyttige funksjonene er rutetider og sanntidsinformasjon om togtrafikken.\n",
      "\n",
      "                Håper dette hjelper!\n"
     ]
    }
   ]
  }
 ]
}
